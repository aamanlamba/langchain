{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30bafa3c",
   "metadata": {},
   "source": [
    "# [Langchain Document loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "186e7ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! source ./.venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72af00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#! uv pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee35b7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents: 1\n",
      "First document content: Oriental Stories was an American pulp magazine published by Popular Fiction and edited by Farnsworth Wright. It was launched in 1930 as a companion to Popular Fiction's Weird Tales, and carried stories with Far Eastern settings, including some fantasy. Contributors included Robert E. Howard, Frank Owen, and E. Hoffmann Price. In 1932 publication was paused; it was relaunched in 1933 under the title The Magic Carpet Magazine, with an expanded editorial policy that now included any story set in an exotic location, including other planets. Some science fiction began to appear alongside the fantasy and adventure material as a result, including work by Edmond Hamilton. Wright obtained stories from H. Bedford Jones, who was a popular pulp writer, and Seabury Quinn. Most of the covers of The Magic Carpet Magazine were by Margaret Brundage. Competition from established pulps in the same niche was too strong, and after five issues under the new title the magazine ceased publication\n"
     ]
    }
   ],
   "source": [
    "## text loader\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "loader = TextLoader(\"../data/input.txt\")\n",
    "documents = loader.load()\n",
    "print(f\"Number of documents: {len(documents)}\")\n",
    "print(f\"First document content: {documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "010f3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of PDF documents: 15\n",
      "First PDF document content: Provided proper attribution is provided, Google hereby grants permission to\n",
      "reproduce the tables and figures in this paper solely for use in journalistic or\n",
      "scholarly works.\n",
      "Attention Is All You Need\n",
      "Ashish Vaswani∗\n",
      "Google Brain\n",
      "avaswani@google.com\n",
      "Noam Shazeer∗\n",
      "Google Brain\n",
      "noam@google.com\n",
      "Niki Parmar∗\n",
      "Google Research\n",
      "nikip@google.com\n",
      "Jakob Uszkoreit∗\n",
      "Google Research\n",
      "usz@google.com\n",
      "Llion Jones∗\n",
      "Google Research\n",
      "llion@google.com\n",
      "Aidan N. Gomez∗ †\n",
      "University of Toronto\n",
      "aidan@cs.toronto.edu\n",
      "Łukasz Kaiser∗\n",
      "Google Brain\n",
      "lukaszkaiser@google.com\n",
      "Illia Polosukhin∗ ‡\n",
      "illia.polosukhin@gmail.com\n",
      "Abstract\n",
      "The dominant sequence transduction models are based on complex recurrent or\n",
      "convolutional neural networks that include an encoder and a decoder. The best\n",
      "performing models also connect the encoder and decoder through an attention\n",
      "mechanism. We propose a new simple network architecture, the Transformer,\n",
      "based solely on attention mechanisms, dispensing with recurrence and convolutions\n",
      "entirely. Experiments on two machine translation tasks show these models to\n",
      "be superior in quality while being more parallelizable and requiring significantly\n",
      "less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-\n",
      "to-German translation task, improving over the existing best results, including\n",
      "ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task,\n",
      "our model establishes a new single-model state-of-the-art BLEU score of 41.8 after\n",
      "training for 3.5 days on eight GPUs, a small fraction of the training costs of the\n",
      "best models from the literature. We show that the Transformer generalizes well to\n",
      "other tasks by applying it successfully to English constituency parsing both with\n",
      "large and limited training data.\n",
      "∗Equal contribution. Listing order is random. Jakob proposed replacing RNNs with self-attention and started\n",
      "the effort to evaluate this idea. Ashish, with Illia, designed and implemented the first Transformer models and\n",
      "has been crucially involved in every aspect of this work. Noam proposed scaled dot-product attention, multi-head\n",
      "attention and the parameter-free position representation and became the other person involved in nearly every\n",
      "detail. Niki designed, implemented, tuned and evaluated countless model variants in our original codebase and\n",
      "tensor2tensor. Llion also experimented with novel model variants, was responsible for our initial codebase, and\n",
      "efficient inference and visualizations. Lukasz and Aidan spent countless long days designing various parts of and\n",
      "implementing tensor2tensor, replacing our earlier codebase, greatly improving results and massively accelerating\n",
      "our research.\n",
      "†Work performed while at Google Brain.\n",
      "‡Work performed while at Google Research.\n",
      "31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.\n",
      "arXiv:1706.03762v7  [cs.CL]  2 Aug 2023\n"
     ]
    }
   ],
   "source": [
    "## pdf loader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "pdf_loader = PyPDFLoader(\"../data/attention.pdf\")\n",
    "pdf_documents = pdf_loader.load()\n",
    "print(f\"Number of PDF documents: {len(pdf_documents)}\")\n",
    "print(f\"First PDF document content: {pdf_documents[0].page_content}\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271d37a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# web base loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "web_loader = WebBaseLoader(\"https://python.langchain.com/docs/integrations/document_loaders/\")\n",
    "web_documents = web_loader.load()\n",
    "print(f\"Number of web documents: {len(web_documents)}\")\n",
    "print(f\"First web document content: {web_documents[0].page_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1f2fcce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CSV documents: 3\n",
      "First CSV document content: name: Alice\n",
      "age: 25\n",
      "city: New York\n"
     ]
    }
   ],
   "source": [
    "# CSV loader\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample CSV file\n",
    "sample_data = pd.DataFrame({\n",
    "    'name': ['Alice', 'Bob', 'Charlie'],\n",
    "    'age': [25, 30, 35],\n",
    "    'city': ['New York', 'London', 'Paris']\n",
    "})\n",
    "sample_data.to_csv('../data/sample.csv', index=False)\n",
    "\n",
    "# Load the CSV file\n",
    "csv_loader = CSVLoader(file_path='../data/sample.csv')\n",
    "csv_documents = csv_loader.load()\n",
    "print(f\"Number of CSV documents: {len(csv_documents)}\")\n",
    "print(f\"First CSV document content: {csv_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6def0ba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of JSON documents: 3\n",
      "First JSON document content: {\"name\": \"Alice\", \"role\": \"Developer\"}\n"
     ]
    }
   ],
   "source": [
    "# JSON loader\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "import json\n",
    "\n",
    "# Create a sample JSON file\n",
    "sample_json = {\n",
    "    'employees': [\n",
    "        {'name': 'Alice', 'role': 'Developer'},\n",
    "        {'name': 'Bob', 'role': 'Designer'},\n",
    "        {'name': 'Charlie', 'role': 'Manager'}\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open('../data/sample.json', 'w') as f:\n",
    "    json.dump(sample_json, f)\n",
    "\n",
    "# Define a simple jq-style function to extract data\n",
    "def extract_data(record: dict):\n",
    "    return record.get('employees', [])\n",
    "\n",
    "# Load the JSON file\n",
    "json_loader = JSONLoader(\n",
    "    file_path='../data/sample.json',\n",
    "    jq_schema='.employees[]',\n",
    "    text_content=False\n",
    ")\n",
    "json_documents = json_loader.load()\n",
    "print(f\"Number of JSON documents: {len(json_documents)}\")\n",
    "print(f\"First JSON document content: {json_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c358b43e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFeatureNotFound\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Load the HTML file\u001b[39;00m\n\u001b[32m     28\u001b[39m html_loader = BSHTMLLoader(\u001b[33m'\u001b[39m\u001b[33m../data/sample.html\u001b[39m\u001b[33m'\u001b[39m, bs_kwargs={\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m: \u001b[33m'\u001b[39m\u001b[33mlxml\u001b[39m\u001b[33m'\u001b[39m})\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m html_documents = \u001b[43mhtml_loader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of HTML documents: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(html_documents)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFirst HTML document content: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhtml_documents[\u001b[32m0\u001b[39m].page_content\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/langchain/.venv/lib/python3.12/site-packages/langchain_core/document_loaders/base.py:43\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     38\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into `Document` objects.\u001b[39;00m\n\u001b[32m     39\u001b[39m \n\u001b[32m     40\u001b[39m \u001b[33;03m    Returns:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        The documents.\u001b[39;00m\n\u001b[32m     42\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/langchain/.venv/lib/python3.12/site-packages/langchain_community/document_loaders/html_bs.py:126\u001b[39m, in \u001b[36mBSHTMLLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbs4\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BeautifulSoup\n\u001b[32m    125\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;28mself\u001b[39m.file_path, \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m, encoding=\u001b[38;5;28mself\u001b[39m.open_encoding) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m--> \u001b[39m\u001b[32m126\u001b[39m     soup = \u001b[43mBeautifulSoup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m text = soup.get_text(\u001b[38;5;28mself\u001b[39m.get_text_separator)\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m soup.title:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/langchain/.venv/lib/python3.12/site-packages/bs4/__init__.py:366\u001b[39m, in \u001b[36mBeautifulSoup.__init__\u001b[39m\u001b[34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[39m\n\u001b[32m    364\u001b[39m     possible_builder_class = builder_registry.lookup(*features)\n\u001b[32m    365\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m possible_builder_class \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m FeatureNotFound(\n\u001b[32m    367\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt find a tree builder with the features you \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    368\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mrequested: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m. Do you need to install a parser library?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    369\u001b[39m             % \u001b[33m\"\u001b[39m\u001b[33m,\u001b[39m\u001b[33m\"\u001b[39m.join(features)\n\u001b[32m    370\u001b[39m         )\n\u001b[32m    371\u001b[39m     builder_class = possible_builder_class\n\u001b[32m    373\u001b[39m \u001b[38;5;66;03m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[32m    374\u001b[39m \u001b[38;5;66;03m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[32m    375\u001b[39m \u001b[38;5;66;03m# with the remaining **kwargs.\u001b[39;00m\n",
      "\u001b[31mFeatureNotFound\u001b[39m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "# HTML loader\n",
    "from langchain_community.document_loaders import BSHTMLLoader\n",
    "import bs4\n",
    "\n",
    "# Create a sample HTML file\n",
    "sample_html = \"\"\"\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Sample Page</title>\n",
    "</head>\n",
    "<body>\n",
    "    <h1>Welcome to the Sample Page</h1>\n",
    "    <p>This is a paragraph of text.</p>\n",
    "    <ul>\n",
    "        <li>Item 1</li>\n",
    "        <li>Item 2</li>\n",
    "        <li>Item 3</li>\n",
    "    </ul>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "with open('../data/sample.html', 'w') as f:\n",
    "    f.write(sample_html)\n",
    "\n",
    "# Load the HTML file\n",
    "html_loader = BSHTMLLoader('../data/sample.html', bs_kwargs={'features': 'lxml'})\n",
    "html_documents = html_loader.load()\n",
    "print(f\"Number of HTML documents: {len(html_documents)}\")\n",
    "print(f\"First HTML document content: {html_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b1079c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of XML documents: 1\n",
      "XML document content: The Great Gatsby\n",
      "\n",
      "F. Scott Fitzgerald\n",
      "\n",
      "1925\n",
      "\n",
      "1984\n",
      "\n",
      "George Orwell\n",
      "\n",
      "1949\n"
     ]
    }
   ],
   "source": [
    "# XML loader\n",
    "from langchain_community.document_loaders import UnstructuredXMLLoader\n",
    "\n",
    "# Create a sample XML file\n",
    "sample_xml = \"\"\"<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<library>\n",
    "    <book>\n",
    "        <title>The Great Gatsby</title>\n",
    "        <author>F. Scott Fitzgerald</author>\n",
    "        <year>1925</year>\n",
    "    </book>\n",
    "    <book>\n",
    "        <title>1984</title>\n",
    "        <author>George Orwell</author>\n",
    "        <year>1949</year>\n",
    "    </book>\n",
    "</library>\"\"\"\n",
    "\n",
    "with open('../data/sample.xml', 'w') as f:\n",
    "    f.write(sample_xml)\n",
    "\n",
    "# Load the XML file\n",
    "xml_loader = UnstructuredXMLLoader('../data/sample.xml')\n",
    "xml_documents = xml_loader.load()\n",
    "print(f\"Number of XML documents: {len(xml_documents)}\")\n",
    "print(f\"XML document content: {xml_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e864284e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Markdown documents: 1\n",
      "Markdown document content: Project Documentation\n",
      "\n",
      "Introduction\n",
      "\n",
      "This is a sample markdown file for testing the document loader.\n",
      "\n",
      "Features\n",
      "\n",
      "Easy to read\n",
      "\n",
      "Simple formatting\n",
      "\n",
      "Supports bold and italic text\n",
      "\n",
      "Code Example\n",
      "\n",
      "def hello_world():\n",
      "    print(\"Hello, World!\")\n"
     ]
    }
   ],
   "source": [
    "# Markdown loader\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "\n",
    "# Create a sample Markdown file\n",
    "sample_markdown = \"\"\"# Project Documentation\n",
    "\n",
    "## Introduction\n",
    "This is a sample markdown file for testing the document loader.\n",
    "\n",
    "### Features\n",
    "- Easy to read\n",
    "- Simple formatting\n",
    "- Supports **bold** and *italic* text\n",
    "\n",
    "## Code Example\n",
    "```python\n",
    "def hello_world():\n",
    "    print(\"Hello, World!\")\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open('../data/sample.md', 'w') as f:\n",
    "    f.write(sample_markdown)\n",
    "\n",
    "# Load the Markdown file\n",
    "md_loader = UnstructuredMarkdownLoader('../data/sample.md')\n",
    "md_documents = md_loader.load()\n",
    "print(f\"Number of Markdown documents: {len(md_documents)}\")\n",
    "print(f\"Markdown document content: {md_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1a767b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|██████████| 3/3 [00:00<00:00, 289.88it/s]libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n",
      "100%|██████████| 3/3 [00:00<00:00, 289.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents loaded from directory: 3\n",
      "\n",
      "Document source: ../data/sample_dir/file2.txt\n",
      "Content: This is the content of file 2.\n",
      "\n",
      "Document source: ../data/sample_dir/file3.txt\n",
      "Content: This is the content of file 3.\n",
      "\n",
      "Document source: ../data/sample_dir/file1.txt\n",
      "Content: This is the content of file 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Directory loader\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "import os\n",
    "\n",
    "# Create a directory with multiple files\n",
    "os.makedirs('../data/sample_dir', exist_ok=True)\n",
    "\n",
    "# Create a few text files in the directory\n",
    "files_content = {\n",
    "    'file1.txt': 'This is the content of file 1.',\n",
    "    'file2.txt': 'This is the content of file 2.',\n",
    "    'file3.txt': 'This is the content of file 3.'\n",
    "}\n",
    "\n",
    "for filename, content in files_content.items():\n",
    "    with open(f'../data/sample_dir/{filename}', 'w') as f:\n",
    "        f.write(content)\n",
    "\n",
    "# Load all text files from the directory\n",
    "dir_loader = DirectoryLoader(\n",
    "    '../data/sample_dir',\n",
    "    glob='**/*.txt',  # Load all .txt files recursively\n",
    "    show_progress=True\n",
    ")\n",
    "dir_documents = dir_loader.load()\n",
    "print(f\"Number of documents loaded from directory: {len(dir_documents)}\")\n",
    "for doc in dir_documents:\n",
    "    print(f\"\\nDocument source: {doc.metadata['source']}\")\n",
    "    print(f\"Content: {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58939867",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libmagic is unavailable but assists in filetype detection. Please consider installing libmagic for better results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of email documents: 1\n",
      "Email document content: Dear User,\n",
      "\n",
      "This is a test email for demonstrating the UnstructuredEmailLoader.\n",
      "\n",
      "Best regards,\n",
      "\n",
      "Sender\n"
     ]
    }
   ],
   "source": [
    "# Email loader\n",
    "from langchain_community.document_loaders import UnstructuredEmailLoader\n",
    "\n",
    "# Create a sample email file\n",
    "sample_email = \"\"\"From: sender@example.com\n",
    "To: recipient@example.com\n",
    "Subject: Test Email\n",
    "Date: Sat, 26 Oct 2025 10:00:00 -0500\n",
    "\n",
    "Dear User,\n",
    "\n",
    "This is a test email for demonstrating the UnstructuredEmailLoader.\n",
    "\n",
    "Best regards,\n",
    "Sender\n",
    "\"\"\"\n",
    "\n",
    "with open('../data/sample.eml', 'w') as f:\n",
    "    f.write(sample_email)\n",
    "\n",
    "# Load the email file\n",
    "email_loader = UnstructuredEmailLoader('../data/sample.eml')\n",
    "email_documents = email_loader.load()\n",
    "print(f\"Number of email documents: {len(email_documents)}\")\n",
    "print(f\"Email document content: {email_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9ee6894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install additional required packages\n",
    "#!uv pip install \"unstructured[all-docs]\" pytesseract \"pdfminer.six>=20221105\" python-pptx openpyxl pillow lxml feedparser tesseract tesseract-ocr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25b8160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install python-dotenv if not already installed\n",
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ea2db234",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'open_filename' from 'pdfminer.utils' (/Users/aamanlamba/Code/langchain/.venv/lib/python3.12/site-packages/pdfminer/utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 14\u001b[39m\n\u001b[32m     11\u001b[39m img.save(\u001b[33m'\u001b[39m\u001b[33m../data/sample_text.png\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Alternative approach using unstructured directly\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_image\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain_core\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocuments\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Load and extract text from the image\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/langchain/.venv/lib/python3.12/site-packages/unstructured/partition/image.py:10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exactly_one\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlang\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m check_language_args\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpdf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m partition_pdf_or_image\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01munstructured\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpartition\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconstants\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PartitionStrategy\n\u001b[32m     14\u001b[39m \u001b[38;5;129m@process_metadata\u001b[39m()  \u001b[38;5;66;03m# TODO(shreya): update to use `apply_metadata` decorator\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;129m@add_metadata\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;129m@add_chunking_strategy\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     37\u001b[39m     **kwargs: Any,\n\u001b[32m     38\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Element]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Code/langchain/.venv/lib/python3.12/site-packages/unstructured/partition/pdf.py:15\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwrapt\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayout\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LTContainer, LTImage, LTItem, LTTextBox\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpdfminer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m open_filename\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpi_heif\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m register_heif_opener\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mPIL\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Image \u001b[38;5;28;01mas\u001b[39;00m PILImage\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'open_filename' from 'pdfminer.utils' (/Users/aamanlamba/Code/langchain/.venv/lib/python3.12/site-packages/pdfminer/utils.py)"
     ]
    }
   ],
   "source": [
    "# Image loader\n",
    "from langchain_community.document_loaders import UnstructuredImageLoader\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import numpy as np\n",
    "\n",
    "# Create a sample image with text\n",
    "img = Image.new('RGB', (400, 100), color='white')\n",
    "d = ImageDraw.Draw(img)\n",
    "text = \"This is sample text in an image\"\n",
    "d.text((10, 40), text, fill='black')\n",
    "img.save('../data/sample_text.png')\n",
    "\n",
    "# Alternative approach using unstructured directly\n",
    "from unstructured.partition.image import partition_image\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "# Load and extract text from the image\n",
    "elements = partition_image(filename='../data/sample_text.png')\n",
    "image_documents = [Document(page_content=\"\\n\\n\".join([str(el) for el in elements]))]\n",
    "print(f\"Number of documents from image: {len(image_documents)}\")\n",
    "print(f\"Extracted text from image: {image_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b3475cbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents from PowerPoint: 1\n",
      "Extracted text from PowerPoint: Sample Presentation\n",
      "\n",
      "This is a bullet point\n",
      "\n",
      "This is another bullet point\n"
     ]
    }
   ],
   "source": [
    "# PowerPoint loader\n",
    "from langchain_community.document_loaders import UnstructuredPowerPointLoader\n",
    "from pptx import Presentation\n",
    "\n",
    "# Create a sample PowerPoint file\n",
    "prs = Presentation()\n",
    "slide = prs.slides.add_slide(prs.slide_layouts[1])\n",
    "title = slide.shapes.title\n",
    "body = slide.shapes.placeholders[1]\n",
    "title.text = \"Sample Presentation\"\n",
    "body.text = \"This is a bullet point\\nThis is another bullet point\"\n",
    "prs.save('../data/sample.pptx')\n",
    "\n",
    "# Load the PowerPoint file\n",
    "pptx_loader = UnstructuredPowerPointLoader('../data/sample.pptx')\n",
    "pptx_documents = pptx_loader.load()\n",
    "print(f\"Number of documents from PowerPoint: {len(pptx_documents)}\")\n",
    "print(f\"Extracted text from PowerPoint: {pptx_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "d9655eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents from Excel: 1\n",
      "Extracted text from Excel: Name Age Department John 30 IT Alice 25 HR Bob 35 Finance\n"
     ]
    }
   ],
   "source": [
    "# Excel loader\n",
    "from langchain_community.document_loaders import UnstructuredExcelLoader\n",
    "import pandas as pd\n",
    "\n",
    "# Create a sample Excel file\n",
    "data = {\n",
    "    'Name': ['John', 'Alice', 'Bob'],\n",
    "    'Age': [30, 25, 35],\n",
    "    'Department': ['IT', 'HR', 'Finance']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_excel('../data/sample.xlsx', index=False)\n",
    "\n",
    "# Load the Excel file\n",
    "excel_loader = UnstructuredExcelLoader('../data/sample.xlsx')\n",
    "excel_documents = excel_loader.load()\n",
    "print(f\"Number of documents from Excel: {len(excel_documents)}\")\n",
    "print(f\"Extracted text from Excel: {excel_documents[0].page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "13f1526b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error processing entry https://www.cnn.com/business/live-news/fox-news-dominion-trial-04-18-23/index.html, exception: newspaper package not found, please install it with `pip install newspaper3k`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of RSS feed items: 0\n",
      "\n",
      "First RSS item content:\n",
      "Error loading RSS feed: list index out of range\n"
     ]
    }
   ],
   "source": [
    "# RSS Feed loader\n",
    "from langchain_community.document_loaders import RSSFeedLoader\n",
    "\n",
    "# Load content from an RSS feed\n",
    "rss_url = \"http://rss.cnn.com/rss/cnn_topstories.rss\"  # Example RSS feed URL\n",
    "rss_loader = RSSFeedLoader(urls=[rss_url])\n",
    "\n",
    "try:\n",
    "    rss_documents = rss_loader.load()\n",
    "    print(f\"Number of RSS feed items: {len(rss_documents)}\")\n",
    "    print(\"\\nFirst RSS item content:\")\n",
    "    print(f\"Title: {rss_documents[0].metadata.get('title', 'No title')}\")\n",
    "    print(f\"Link: {rss_documents[0].metadata.get('link', 'No link')}\")\n",
    "    print(f\"Content: {rss_documents[0].page_content[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading RSS feed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419350b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GitHub Token loaded: No\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv  # Correct import from python-dotenv package\n",
    "load_dotenv()\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "print(f\"GitHub Token loaded: {'Yes' if github_token else 'No'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aade1ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To access GitHub, you'll need a GitHub Personal Access Token\n",
      "Create one at https://github.com/settings/tokens with 'repo' scope\n",
      "None\n",
      "Error loading GitHub issues: 1 validation error for GitHubIssuesLoader\n",
      "  Value error, Did not find access_token, please add an environment variable `GITHUB_PERSONAL_ACCESS_TOKEN` which contains it, or pass `access_token` as a named parameter. [type=value_error, input_value={'repo': 'langchain-ai/la...pen', 'labels': ['bug']}, input_type=dict]\n",
      "    For further information visit https://errors.pydantic.dev/2.12/v/value_error\n",
      "Make sure your token has the necessary permissions\n"
     ]
    }
   ],
   "source": [
    "# GitHub loader\n",
    "from langchain_community.document_loaders import GitHubIssuesLoader\n",
    "\n",
    "# To use the GitHub loader, you'll need a GitHub access token\n",
    "# You can create one at https://github.com/settings/tokens\n",
    "print(\"Using GitHub token from environment variables...\")\n",
    "\n",
    "# Load the token from environment variables\n",
    "import os\n",
    "from dotenv import load_dotenv  # Correct import from python-dotenv package\n",
    "load_dotenv()\n",
    "github_token = os.getenv(\"GITHUB_TOKEN\")\n",
    "\n",
    "if not github_token:\n",
    "    raise ValueError(\"GitHub token not found in environment variables. Please check your .env file.\")\n",
    "\n",
    "# Load issues from a GitHub repository\n",
    "repo = \"langchain-ai/langchain\"  # Example repository\n",
    "try:\n",
    "    github_loader = GitHubIssuesLoader(\n",
    "        repo=repo,\n",
    "        access_token=github_token,\n",
    "        state=\"open\",  # can be \"open\", \"closed\", or \"all\"\n",
    "        labels=[\"bug\"],  # filter issues by labels\n",
    "    )    \n",
    "    github_documents = github_loader.load()\n",
    "    print(f\"\\nNumber of GitHub issues loaded: {len(github_documents)}\")\n",
    "    print(\"\\nFirst issue content:\")\n",
    "    print(f\"Title: {github_documents[0].metadata.get('title', 'No title')}\")\n",
    "    print(f\"State: {github_documents[0].metadata.get('state', 'Unknown')}\")\n",
    "    print(f\"URL: {github_documents[0].metadata.get('url', 'No URL')}\")\n",
    "    print(f\"Content preview: {github_documents[0].page_content[:200]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading GitHub issues: {e}\")\n",
    "    print(\"Make sure your token has the necessary permissions\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
